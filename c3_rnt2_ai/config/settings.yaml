profiles:
  dev_small:
    tokenizer:
      rnt2_model_path: data/runs/rnt2_dev.pt
      vortex_model_path: data/runs/vortex_tok.pt
      block_size: 64
      escape_mode: exact
      macro_codebook_size: 256
      macro_min_len: 2
    core:
      hidden_size: 256
      layers: 4
      heads: 4
      vocab_size: 1024
      mtp_k: 0
      compile_step: false
      compile_local_mixer_step: false
    vortex_model:
      window_size: 128
      latent_slots: 64
      lava_top_k: 4
      lava_clusters: 0
      lava_cluster_top: 1
      lava_read_every: 1
      lava_write_every: 1
      lava_write_on_surprise: false
      lava_surprise_threshold: 0.0
      lava_state_path: data/memory/lava_state.pt
      lava_state_autoload: false
      local_mixer_kernel: 5
      ssm_state_size: 128
      gated_mlp_ratio: 4
    bad:
      block_size: 8
      entropy_threshold: 3.5
      entropy_top_k: 64
      penalty_window: 512
      top_p_min_k: 128
      top_p_max_k: 512
      exact_copy_mode: false
      escape_restrict: false
      use_mtp: true
    kv:
      window_size: 128
      kv_quant_bits: 8
      latent_slots: 32
    c3:
      tile_size: 128
      cache_vram_budget_mb: 2048
      prefetch_depth: 2
      compression: zstd
      pinned_memory: true
    depth_gating:
      enabled: false
      min_depth: 2
      max_depth: 4
      hysteresis: 0.2
      entropy_threshold: 3.5
      entropy_top_k: 64
      compute_cost_weight: 0.0
      smoothness: 1.0
    router:
      top_k: 2
      stability_threshold: 0.1
      mem_cost_weight: 0.01
      enabled: false
      weights:
        speed: 1.0
        vram: 0.01
        error: 5.0
        quality: 1.0
    agent:
      web_allowlist: ["github.com", "docs.python.org", "pytorch.org", "duckduckgo.com"]
      tools_enabled: ["run_tests", "search_web", "open_docs", "propose_patch", "sandbox_patch", "apply_patch", "summarize_diff", "read_file", "grep", "list_tree"]
    tools:
      web:
        enabled: false
        allow_domains: ["github.com", "docs.python.org", "pytorch.org"]
        search_domains: ["duckduckgo.com"]
        max_bytes: 512000
        timeout_s: 10
        rate_limit_per_min: 30
        cache_dir: data/web_cache
        cache_ttl_s: 3600
        allow_content_types: ["text/", "application/json"]
    adapters:
      enabled: false
      max_loaded: 2
      default: null
      paths: {}
      router:
        mode: keyword_map
        keyword_map: {}
        top_k: 1
        mix_mode: single
    knowledge:
      embedding_backend: auto
      embedding_model: sentence-transformers/all-MiniLM-L6-v2
      index_backend: auto
      policy:
        min_quality: 0.0
        max_age_days: null
        allow_domains: null
        deny_domains: null
        allow_source_kinds: null
        deny_source_kinds: null
    self_patch:
      enabled: false
      auto_sandbox: true
      queue_dir: data/self_patch/queue
      sandbox_dir: data/self_patch/sandbox
      max_patch_kb: 128
      run_tests_on_apply: true
      allowed_commands: ["pytest", "ruff", "python"]
      allowed_paths: ["src/", "tests/"]
      forbidden_globs: [".env", ".env.*", "data/**", "*.key", "*.pem", "*.p12", "*.sqlite", "*.db", "keys/**", "secrets/**", "src/c3rnt2/self_patch/**", "src/c3rnt2/selfimprove/**"]
    continuous:
      interval_minutes: 30
      adapter_rank: 4
      max_steps: 50
      knowledge_path: data/continuous/knowledge.sqlite
      ingest_web: true
      ingest_urls:
        - "https://docs.python.org/3/"
        - "https://pytorch.org/docs/stable/"
      web_discovery:
        enabled: false
        seed_queries: []
        max_urls_per_tick: 10
      ingest:
        max_files_per_tick: 100
        max_bytes_per_file: 1048576
        max_total_bytes_per_tick: 5242880
        web:
          cooldown_minutes: 60
          sanitize:
            max_chars: 2000
            max_instruction_density: 0.04
            max_repeat_lines: 2
      trigger:
        enabled: true
        min_new_docs: 1
        min_novelty: 0.2
        min_successes: 0
      filter:
        min_quality: 0.35
        max_repeat_ratio: 0.8
        min_chars: 200
        min_novelty: 0.2
        quarantine_limit: 200
      replay:
        path: data/continuous/replay.sqlite
        max_items: 2000
        top_frac: 0.7
        random_frac: 0.3
        sample_size: 64
        seed_chunks: 40
      source_weights:
        episode: 2.0
        web: 1.0
        memory: 0.8
        logs: 0.3
        unknown: 1.0
      eval:
        anchors_path: data/continuous/anchors.jsonl
        max_regression: 0.2
        min_improvement: 0.0
      adapters:
        rank: 4
        alpha: 1.0
        target_modules: ["lm_head", "fc", "proj", "gate", "read_proj", "addr_proj", "out_proj", "in_proj"]
        strict_target_modules: false
      consolidation:
        enabled: false
        every_n_runs: 10
        top_n: 3
        weights:
          loss: 1.0
          anchors: 1.0
          episodes: 0.2
    rag:
      enabled: false
      top_k: 3
      max_chars: 1200
    selfimprove:
      max_patch_kb: 64
      sandbox_root: data/workspaces
  qwen8b_base:
    base_profile: dev_small
    core:
      backend: hf
      hf_model: "Qwen/Qwen2.5-7B-Instruct"
      # Avoid OOM/thrashing on Windows when 4-bit quant (bitsandbytes) isn't available:
      # keep VRAM under bench.max_vram_peak_mb via CPU offload.
      hf_device_map: auto
      hf_max_memory:
        0: "12GiB"
        cpu: "48GiB"
      hf_offload_folder: "data/hf_offload"
      hf_use_safetensors: true
      hf_load_in_4bit: true
      hf_device: cuda
      hf_attn_implementation: sdpa
      hf_system_prompt: "You are Vortex, a helpful coding assistant."
      hf_use_latest_adapter: true
      dtype: bf16
      backend_fallback: vortex
      vram_threshold_mb: 1200
      vram_floor_tokens: 32
      vram_ceil_tokens: 512
      vram_safety_margin_mb: 512
    server:
      auto_reload_adapter: false
      reload_interval_s: 60
      maintenance_window_s: 10
    decode:
      max_new_tokens: 128
    runtime:
      paged_lm_head: false
      # gpu_decompress uses CPU decompress + H2D pipeline; keep "none" unless you know why.
      gpu_decompress: none
      # kv_quant=2bit is experimental; requires kv_quant_2bit_experimental + i_know_what_im_doing.
      i_know_what_im_doing: false

  rtx4080_16gb:
    # RTX 4080 16GB + Ryzen 7800X3D (HF Qwen-8B)
    base_profile: qwen8b_base
    core:
      hf_load_in_4bit: true
      hf_device: cuda
      hf_attn_implementation: sdpa
      dtype: bf16
      backend_fallback: vortex
    decode:
      max_new_tokens: 256

  qwen8b_train:
    base_profile: qwen8b_base
    server:
      block_during_training: true
      maintenance_window_s: 10
      train_strategy: subprocess
    tools:
      web:
        enabled: true
    hf_train:
      enabled: true
      model_name: "Qwen/Qwen2.5-7B-Instruct"
      registry_dir: data/registry/hf_train
      dataset_path: data/registry/hf_train/sft_samples.jsonl
      state_path: data/registry/hf_train/state.json
      max_samples: 128
      auto_tune_batch: true
      auto_tune_retries: 2
      min_quality: 0.3
      prompt_template: "Context:\n{text}\nAnswer:"
      max_seq_len: 1024
      micro_batch_size: 1
      grad_accum_steps: 8
      max_steps: 100
      lr: 2.0e-4
      load_in_4bit: true
      load_in_8bit: false
      quant_type: nf4
      double_quant: true
      compute_dtype: bf16
      gradient_checkpointing: true
      lora_rank: 8
      lora_alpha: 16
      lora_dropout: 0.05
      target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
      use_weighted_sampling: true
      source_kind_weights:
        chat_feedback: 1.5
        chat_feedback_soft: 0.7
        episode: 1.2
        web: 0.8
        logs: 0.8

  safe_selftrain_4080_hf:
    # HF Qwen-8B self-train (safe defaults for 16GB)
    base_profile: qwen8b_train
    core:
      backend: hf
      hf_model: "Qwen/Qwen2.5-7B-Instruct"
    server:
      train_strategy: subprocess_unload
      block_during_training: true
      auto_reload_adapter: true
      reload_interval_s: 30
      maintenance_window_s: 10
    tools:
      web:
        enabled: false
    continuous:
      ingest_web: false
    autopilot:
      enabled: false
      interval_minutes: 30
      ingest_cooldown_minutes: 10
      train_cooldown_minutes: 60
      eval_cooldown_minutes: 60
      patch_cooldown_minutes: 120
      train_max_steps: 25
      training_jsonl_max_items: 500
      min_improvement: 0.0
      reuse_dataset: false
      autopatch_enabled: false
      autopatch_on_test_fail: true
      autopatch_on_doctor_fail: true
      autopatch_require_eval: true
      todo_regex: "TODO\\((P1|PRIORITY)\\)|TODO!|TODO:HIGH|TODO:CRITICAL"

  rtx4080_16gb_safe_hf:
    # Alias: safe HF self-train profile for RTX 4080 16GB
    base_profile: safe_selftrain_4080_hf
    decode:
      max_new_tokens: 256
    bench_thresholds:
      min_tokens_per_sec: 10.0
      max_regression: 0.15

  rtx4080_16gb_safe:
    # RTX 4080 16GB safe core profile (no downloads, deny-by-default web).
    base_profile: rtx4080_16gb_safe_vortex
    core:
      precision: bf16
    kv:
      window_size: 1024
    runtime:
      paged_lm_head: true
      cache_vram_budget_mb: 3072
      prefetch_depth: 2
      kv_quant: int8
      kv_quant_2bit_experimental: false
      paged_lm_head_stream_topk: false
    decode:
      max_new_tokens: 64
    continuous:
      ingest_web: false
      batch_tokens: 4096
    tools:
      web:
        enabled: false
    security:
      web:
        strict: true
        allowlist_domains: []

  rtx4080_16gb_safe_windows_hf:
    # Windows-safe RTX 4080 16GB (HF). Defaults: safe, offline, fail-closed promotion.
    base_profile: qwen8b_train
    core:
      backend: hf
      hf_model: "Qwen/Qwen2.5-7B-Instruct"
    tools:
      web:
        enabled: false
    continuous:
      ingest_web: false
    server:
      train_strategy: subprocess_unload
      block_during_training: true
      auto_reload_adapter: true
      reload_interval_s: 30
    hf_train:
      enabled: false
      max_seq_len: 2048
      micro_batch_size: 1
      grad_accum_steps: 8
      lora_rank: 8
      eval:
        enabled: true
    autopilot:
      enabled: false
      autopatch_enabled: false
    learning:
      require_eval_ok: true
      require_bench_ok: true
    experts:
      enabled: false
      max_loaded: 6
      default: null
      paths: {}
      router:
        mode: hybrid
        keyword_map: {}
    bench:
      required_ctx: 4096
      max_regression_pct: 15
      max_vram_peak_mb: 15500
      min_tokens_per_sec: 0

  rtx4080_16gb_safe_windows_core:
    # Windows-safe RTX 4080 16GB (core backend).
    base_profile: rtx4080_16gb_safe_vortex
    tools:
      web:
        enabled: false
    continuous:
      ingest_web: false
    autopilot:
      enabled: false
      autopatch_enabled: false
    experts:
      enabled: false
      max_loaded: 6
      default: null
      paths: {}
      router:
        mode: hybrid
        keyword_map: {}
    bench:
      required_ctx: 4096
      max_regression_pct: 15
      max_vram_peak_mb: 15500
      min_tokens_per_sec: 0

  rtx4080_16gb_safe_windows_llama_cpp:
    # Windows-safe RTX 4080 16GB (llama.cpp / GGUF).
    base_profile: rtx4080_16gb_safe_windows_core
    core:
      backend: llama_cpp
      llama_cpp_model_path: "data/models/<model>.gguf"
      llama_cpp_ctx: 4096
      llama_cpp_n_gpu_layers: -1
      llama_cpp_threads: 8
      llama_cpp_batch: 256
      llama_cpp_flash_attn: false
    tools:
      web:
        enabled: false
    continuous:
      ingest_web: false
    security:
      web:
        strict: true
        allowlist_domains: []
    bench:
      required_ctx: 4096
      max_regression_pct: 15
      max_vram_peak_mb: 15500
      min_tokens_per_sec: 0

  autonomous_4080_hf:
    # Full autonomous (internet + self-train + autopatch) for 4080 16GB.
    # Requires explicit approval file to apply/merge patches.
    base_profile: safe_selftrain_4080_hf
    continuous:
      ingest_web: true
      web_discovery:
        enabled: true
        seed_queries:
          - "Qwen2.5 finetune qlora tips"
          - "FastAPI streaming SSE best practices"
          - "PyTorch CUDA OOM mitigation"
          - "LoRA merge adapter safety"
        ttl_hours: 72
        max_urls_per_tick: 10
        max_queue: 500
        max_crawl_pages_per_tick: 2
        max_links_per_page: 50
        max_sitemap_urls: 200
    autopilot:
      enabled: true
      autopatch_enabled: true
      restart_after_patch: true
      autopatch_strategy: subprocess_cpu
      autopatch_require_approval: true
      approval_file: data/APPROVE_AUTOPATCH
      bench_enabled: true
      bench_max_new_tokens: 64
      bench_max_regression: 0.15
      bench_min_tokens_per_sec: 10.0
      min_new_samples_per_tick: 10
      max_consecutive_failures: 3
      safe_mode_cooldown_minutes: 0
    learning:
      require_eval_ok: true
      require_bench_ok: true
    tools:
      web:
        enabled: true
    server:
      train_strategy: subprocess_unload
      reload_request_interval_s: 2
    self_patch:
      enabled: true

  core_only:
    tokenizer:
      rnt2_model_path: data/runs/rnt2_dev.pt
      vortex_model_path: data/runs/vortex_tok.pt
      block_size: 64
      escape_mode: exact
      macro_codebook_size: 256
      macro_min_len: 2
    core:
      hidden_size: 512
      layers: 6
      heads: 8
      vocab_size: 2048
      mtp_k: 0
      compile_step: false
      compile_local_mixer_step: false
    vortex_model:
      window_size: 256
      latent_slots: 128
      lava_top_k: 4
      lava_clusters: 0
      lava_cluster_top: 1
      lava_read_every: 1
      lava_write_every: 1
      lava_write_on_surprise: false
      lava_surprise_threshold: 0.0
      lava_state_path: data/memory/lava_state.pt
      lava_state_autoload: false
      local_mixer_kernel: 7
      ssm_state_size: 256
      gated_mlp_ratio: 4
    bad:
      block_size: 8
      entropy_threshold: 3.5
      entropy_top_k: 64
      penalty_window: 512
      top_p_min_k: 128
      top_p_max_k: 512
      exact_copy_mode: false
      escape_restrict: false
      use_mtp: true
    kv:
      window_size: 256
      kv_quant_bits: 8
      latent_slots: 64
    depth_gating:
      enabled: false
      min_depth: 2
      max_depth: 6
      hysteresis: 0.2
      entropy_threshold: 3.5
      entropy_top_k: 64
      compute_cost_weight: 0.0
      smoothness: 1.0
  c3_paged:
    tokenizer:
      rnt2_model_path: data/runs/rnt2_dev.pt
      vortex_model_path: data/runs/vortex_tok.pt
      block_size: 64
      escape_mode: exact
      macro_codebook_size: 256
      macro_min_len: 2
    core:
      hidden_size: 512
      layers: 6
      heads: 8
      vocab_size: 2048
      mtp_k: 0
      compile_step: false
      compile_local_mixer_step: false
    vortex_model:
      window_size: 256
      latent_slots: 128
      lava_top_k: 4
      lava_clusters: 0
      lava_cluster_top: 1
      lava_read_every: 1
      lava_write_every: 1
      lava_write_on_surprise: false
      lava_surprise_threshold: 0.0
      lava_state_path: data/memory/lava_state.pt
      lava_state_autoload: false
      local_mixer_kernel: 7
      ssm_state_size: 256
      gated_mlp_ratio: 4
    bad:
      block_size: 8
      entropy_threshold: 3.5
      entropy_top_k: 64
      penalty_window: 512
      top_p_min_k: 128
      top_p_max_k: 512
      exact_copy_mode: false
      escape_restrict: false
      use_mtp: true
    kv:
      window_size: 256
      kv_quant_bits: 8
      latent_slots: 64
    c3:
      tile_size: 128
      cache_vram_budget_mb: 4096
      paged_lm_head_stream_topk: 64
      prefetch_depth: 4
      compression: zstd
      pinned_memory: true
    depth_gating:
      enabled: false
      min_depth: 2
      max_depth: 6
      hysteresis: 0.2
      entropy_threshold: 3.5
      entropy_top_k: 64
      compute_cost_weight: 0.0
      smoothness: 1.0
    router:
      top_k: 4
      stability_threshold: 0.15
      mem_cost_weight: 0.02
      enabled: false
      weights:
        speed: 1.0
        vram: 0.01
        error: 5.0
        quality: 1.0
    continuous:
      interval_minutes: 30
      adapter_rank: 4
      max_steps: 50
    selfimprove:
      max_patch_kb: 64
      sandbox_root: data/workspaces
  agent:
    tokenizer:
      rnt2_model_path: data/runs/rnt2_dev.pt
      vortex_model_path: data/runs/vortex_tok.pt
      block_size: 64
      escape_mode: exact
      macro_codebook_size: 256
      macro_min_len: 2
    core:
      hidden_size: 256
      layers: 4
      heads: 4
      vocab_size: 1024
      mtp_k: 0
      compile_step: false
      compile_local_mixer_step: false
    vortex_model:
      window_size: 128
      latent_slots: 64
      lava_top_k: 4
      lava_clusters: 0
      lava_cluster_top: 1
      lava_read_every: 1
      lava_write_every: 1
      lava_write_on_surprise: false
      lava_surprise_threshold: 0.0
      lava_state_path: data/memory/lava_state.pt
      lava_state_autoload: false
      local_mixer_kernel: 5
      ssm_state_size: 128
      gated_mlp_ratio: 4
    bad:
      block_size: 8
      entropy_threshold: 3.5
      entropy_top_k: 64
      penalty_window: 512
      top_p_min_k: 128
      top_p_max_k: 512
      exact_copy_mode: false
      escape_restrict: false
      use_mtp: true
    kv:
      window_size: 128
      kv_quant_bits: 8
      latent_slots: 32
    depth_gating:
      enabled: false
      min_depth: 2
      max_depth: 4
      hysteresis: 0.2
      entropy_threshold: 3.5
      entropy_top_k: 64
      compute_cost_weight: 0.0
      smoothness: 1.0
    agent:
      web_allowlist: ["github.com", "docs.python.org", "pytorch.org", "duckduckgo.com"]
      tools_enabled: ["run_tests", "search_web", "open_docs", "propose_patch", "sandbox_patch", "apply_patch", "summarize_diff", "read_file", "grep", "list_tree"]
    tools:
      web:
        enabled: false
        allow_domains: ["github.com", "docs.python.org", "pytorch.org"]
        search_domains: ["duckduckgo.com"]
        max_bytes: 512000
        timeout_s: 10
        rate_limit_per_min: 30
        cache_dir: data/web_cache
        cache_ttl_s: 3600
        allow_content_types: ["text/", "application/json"]
    self_patch:
      enabled: false
      auto_sandbox: true
      queue_dir: data/self_patch/queue
      sandbox_dir: data/self_patch/sandbox
      max_patch_kb: 128
      run_tests_on_apply: true
      allowed_commands: ["pytest", "ruff", "python"]
      allowed_paths: ["src/", "tests/"]
      forbidden_globs: [".env", ".env.*", "data/**", "*.key", "*.pem", "*.p12", "*.sqlite", "*.db", "keys/**", "secrets/**", "src/c3rnt2/self_patch/**", "src/c3rnt2/selfimprove/**"]
    continuous:
      interval_minutes: 30
      adapter_rank: 4
      max_steps: 50
    selfimprove:
      max_patch_kb: 64
      sandbox_root: data/workspaces
  rtx4080_16gb_vortexx:
    # Vortex-Tok (reversible, tasa variable, ESC exacto)
    tokenizer:
      type: vortex_tok
      rnt2_model_path: data/runs/rnt2_dev.pt
      vortex_tok_path: data/runs/vortex_tok.pt
      block_size: 64
      escape_mode: exact
      macro:
        enabled: true
        macro_codebook_path: data/runs/vortex_macro.pt
        max_merge: 8
        min_freq: 50

    # Vortex core (V-Blocks + LAVA + SSM)
    core:
      arch: vortex_x
      precision: bf16
      compile: true
      compile_step: false
      compile_local_mixer_step: false
      hidden_size: 768
      layers: 16
      heads: 12
      mlp_ratio: 4
      dropout: 0.0
      mtp_k: 4

      local_window: 1024
      lava_slots: 256
      lava_top_k: 8
      lava_clusters: 64
      lava_cluster_top: 2
      lava_read_every: 1
      lava_write_every: 1
      lava_write_on_surprise: false
      lava_surprise_threshold: 0.0
      lava_state_path: data/memory/lava_state.pt
      lava_state_autoload: false
      lava_addr_dim: 128

      ssm:
        enabled: true
        state_dim: 256
        scan_chunk: 256

    # KV antiguo: úsalo solo como “ventana local” si lo mantienes
    kv:
      window_size: 1024
      kv_quant_bits: 8
      latent_slots: 0

    # Runtime “VRAM como caché” (C3 tiles)
    c3:
      tile_size: 128
      cache_vram_budget_mb: 10000
      prefetch_depth: 4
      compression: zstd
      pinned_memory: true

    router:
      top_k: 4
      stability_threshold: 0.15
      mem_cost_weight: 0.02
      enabled: false
      weights:
        speed: 1.0
        vram: 0.01
        error: 5.0
        quality: 1.0
      hysteresis: 0.1

    decode:
      method: bad
      draft_block: 8
      verify: true
      temperature: 0.7
      top_p: 0.95
      exact_copy_mode: false
      escape_restrict: false
      use_mtp: true
      max_new_tokens: 512
      adaptive_granularity: true
      entropy_top_k: 64
      penalty_window: 512
      top_p_min_k: 128
      top_p_max_k: 512

    depth_gating:
      enabled: true
      min_depth: 8
      max_depth: 16
      hysteresis: 0.2
      entropy_threshold: 3.5
      entropy_top_k: 64
      compute_cost_weight: 0.0
      smoothness: 1.0

    agent:
      web_allowlist:
        - "github.com"
        - "docs.python.org"
        - "pytorch.org"
        - "duckduckgo.com"
        - "developer.nvidia.com"
      tools_enabled: ["run_tests", "search_web", "open_docs", "propose_patch"]
      rate_limit_per_min: 30

  rtx4080_16gb_safe_vortex:
    # Safer defaults for RTX 4080 16GB (core backend)
    base_profile: rtx4080_16gb_vortexx
    core:
      compile: false
      compile_step: false
      compile_local_mixer_step: false
    c3:
      cache_vram_budget_mb: 4096
      prefetch_depth: 2
    decode:
      max_new_tokens: 256
    continuous:
      ingest_web: false
    tools:
      web:
        enabled: false
    server:
      train_strategy: subprocess_unload
      block_during_training: true
    bench_thresholds:
      min_tokens_per_sec: 5.0
      max_regression: 0.2

  rtx4080_16gb_vortexx_next:
    base_profile: rtx4080_16gb_vortexx
    core:
      cuda_graphs: true
    vortex_model:
      lava_ann_mode: ivf
      lava_cluster_ema: 0.1
      lava_cluster_reassign_threshold: 0.05
      lava_shared_groups: 4
      lava_write_every: 2
    decode:
      draft_model:
        enabled: true
        draft_layers: 6
        draft_hidden: 512
        share_embeddings: true
        share_lm_head: true
    runtime:
      paged_lm_head: true
      paged_tile_out: 256
      paged_tile_in: 768
      cache_vram_budget_mb: 4096
      paged_lm_head_stream_topk: 64
      prefetch_depth: 4
      compression: zstd
      pinned_memory: true
      # gpu_decompress uses CPU decompress + H2D pipeline; keep "none" unless you know why.
      gpu_decompress: none
      # kv_quant=2bit is experimental; requires kv_quant_2bit_experimental + i_know_what_im_doing.
      i_know_what_im_doing: false

  safe_selftrain_4080:
    # Vortex continuous self-train (core trainer), not HF
    base_profile: rtx4080_16gb_vortexx
    server:
      block_during_training: true

    continuous:
      enabled: true
      interval_minutes: 30
      max_steps_per_tick: 200
      lr: 1.0e-4
      batch_tokens: 8192
      knowledge_path: data/continuous/knowledge.sqlite
      ingest_web: true
      ingest:
        max_files_per_tick: 200
        max_bytes_per_file: 2097152
        max_total_bytes_per_tick: 10485760
        web:
          cooldown_minutes: 60
          sanitize:
            max_chars: 2000
            max_instruction_density: 0.04
            max_repeat_lines: 2
      adapters:
        type: lora
        rank: 16
        alpha: 32
        target_modules: ["linear", "proj"]
        strict_target_modules: false
      trigger:
        enabled: true
        min_new_docs: 1
        min_novelty: 0.2
        min_successes: 0
      filter:
        min_quality: 0.35
        max_repeat_ratio: 0.8
        min_chars: 200
        min_novelty: 0.2
        quarantine_limit: 200
      replay:
        path: data/continuous/replay.sqlite
        max_items: 5000
        top_frac: 0.7
        random_frac: 0.3
        sample_size: 128
        seed_chunks: 80
      source_weights:
        episode: 2.0
        web: 1.0
        memory: 0.8
        logs: 0.3
        unknown: 1.0
      eval:
        min_improvement: 0.5
        max_regression: 0.2
        anchors_path: data/continuous/anchors.jsonl
        metrics: ["loss", "exact_copy", "json_valid", "tokens_per_sec"]
      consolidation:
        enabled: false
        every_n_runs: 10
        top_n: 3
        weights:
          loss: 1.0
          anchors: 1.0
          episodes: 0.2
      registry_dir: data/registry
      dataset_dir: data/datasets
      safety:
        forbid_self_patch_during_train: true

  selfimprove_sandbox_4080:
    base_profile: rtx4080_16gb_vortexx

    selfimprove:
      enabled: true
      sandbox_dir: data/workspaces/selfimprove
      runs_dir: data/selfimprove/runs
      require_approval: true
      approval_file: data/APPROVE_SELF_PATCH

      # política de edición (safety kernel)
      allowed_paths:
        - "c3_rnt2_ai/src/c3rnt2/"
        - "c3_rnt2_ai/tests/"
        - "c3_rnt2_ai/config/"
      forbidden_paths:
        - "c3_rnt2_ai/src/c3rnt2/selfimprove/safety_kernel.py"
        - ".git/"
      max_patch_kb: 256
      commands_allowed: ["pytest -q"]
