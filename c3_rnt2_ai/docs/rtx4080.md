# RTX 4080 16GB: ruta recomendada

<!-- markdownlint-disable MD013 -->

## Perfil “120B-like” (expert bank)

Perfil recomendado para “120B-like”: `rtx4080_16gb_120b_like`.
Guía completa: `docs/120b_like.md`.

Self-train HF en Windows (WSL2): `docs/WINDOWS_SELF_TRAIN_WSL.md`.

```bash
python -m vortex doctor --deep --mock --profile rtx4080_16gb_120b_like
python -m vortex bench  --mock --profile rtx4080_16gb_120b_like --max-new-tokens 16
python -m vortex serve        --profile rtx4080_16gb_120b_like
```

## Install

```bash
python -m venv .venv
.venv\\Scripts\\activate
pip install -e .
pip install -e .[api]
pip install -e .[hf,train]   # opcional (HF + QLoRA)
```

## Doctor (sin bajar pesos HF)

```bash
python -m vortex doctor --profile rtx4080_16gb_vortexx --deep
python -m vortex doctor --profile qwen8b_base --deep
```

## Chat / Serve

```bash
python -m vortex chat  --profile rtx4080_16gb_vortexx
python -m vortex serve --profile rtx4080_16gb_vortexx

python -m vortex chat  --profile qwen8b_base
python -m vortex serve --profile qwen8b_base
```

## Bench (unificado)

```bash
python -m vortex bench --profile rtx4080_16gb_vortexx --max-new-tokens 64
python -m vortex bench --profile qwen8b_base --max-new-tokens 64
```

## Self-train (serve-self-train)

HF (QLoRA):

```bash
python -m vortex serve-self-train --profile rtx4080_16gb_safe_hf --host 0.0.0.0 --port 8000
```

Core (Vortex):

```bash
python -m vortex serve-self-train --profile safe_selftrain_4080 --host 0.0.0.0 --port 8000
```

## Autopilot (internet + autopatch seguro)

Perfil recomendado (gated por approval file):

```bash
python -m vortex serve-autopilot --profile autonomous_4080_hf --host 0.0.0.0 --port 8000
```

Para permitir aplicar/mergear parches (por defecto **NO** aplica nada):

```bash
type NUL > data\\APPROVE_AUTOPATCH
```

Runner con rollback automático (si autopilot pide restart o falla):

```bash
python scripts\\run_daemon.py --profile autonomous_4080_hf --host 0.0.0.0 --port 8000
```
